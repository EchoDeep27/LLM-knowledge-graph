{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from docling.document_converter import DocumentConverter\n",
    "load_dotenv() \n",
    "\n",
    "\n",
    "files = [\n",
    "    \"Hotel Marketing.pdf\"\n",
    "]\n",
    "\n",
    "pdf_filename =  files[0]\n",
    "\n",
    "\n",
    "\"\"\" Testing: markdown extraction with docling\"\"\"\n",
    "print(torch.cuda.is_available())   \n",
    " \n",
    "\n",
    "def write_file(filename: str, content: str):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    " \n",
    "attempt = 5\n",
    "\n",
    "FOLDER = \"output(docling)\"\n",
    "Path(FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "table_data = []\n",
    "\n",
    "for filename in files:\n",
    "    filepath = f\"pdf/{filename}\"\n",
    "    \n",
    "    time_taken_list = []\n",
    "    for _ in range(attempt):   \n",
    "        start_time = datetime.now()\n",
    "\n",
    "        converter = DocumentConverter()\n",
    "        result = converter.convert(filepath)\n",
    "        markdown_content = result.document.export_to_markdown()\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        time_taken = end_time - start_time\n",
    "        time_taken_list.append(time_taken)\n",
    "\n",
    "        md_filename = filename.replace(\".pdf\", \".md\")\n",
    "        write_file(f\"{FOLDER}/{md_filename}\", markdown_content)\n",
    " \n",
    "    avg_time_taken = sum(time_taken_list, timedelta()) / len(time_taken_list)\n",
    "    table_data.append((filename, time_taken_list, avg_time_taken))\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing hasn't finished yet. Please try later\n"
     ]
    }
   ],
   "source": [
    "markdown_table = \"| File Name                                    | Time Taken (1st) | Time Taken (2nd) | Time Taken (3rd) | Time Taken (4th) | Time Taken (5th) | Average Time Taken |\\n\"\n",
    "markdown_table += \"|----------------------------------------------|------------------|------------------|------------------|------------------|------------------|--------------------|\\n\"\n",
    "\n",
    "for file, time_taken_list, avg_time_taken in table_data:\n",
    "    time_taken_str = \" | \".join([get_time_str(time) for time in time_taken_list])\n",
    "    markdown_table += f\"| {file} | {time_taken_str} | {get_time_str(avg_time_taken)} |\\n\"\n",
    "\n",
    "print(markdown_table)\n",
    "\n",
    "write_file(f\"{FOLDER}/time_taken_summary.md\", markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU with PyTorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/hm3/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import torch \n",
    "from igraph import Graph, plot\n",
    "from typing import Tuple, Optional, Any\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU with PyTorch\")\n",
    " \n",
    "region_name = os.environ.get(\"AWS_REGION\") \n",
    "aws_access_key_id = os.environ.get(\"AWS_ACCESS_KEY_ID\") \n",
    "aws_secret_access_key = os.environ.get(\"AWS_SECRET_ACCESS_KEY\") \n",
    "\n",
    "bedrock_model = os.environ.get(\"BEDROCK_MODEL_NAME\")\n",
    "modelId = os.environ[\"BEDROCK_MODEL_NAME\"]\n",
    "\n",
    "OLLAMA_BASE_URL = os.environ[\"OLLAMA_BASE_URL\"]     \n",
    "\n",
    "\n",
    "client = boto3.client('bedrock-runtime', \n",
    "                      region_name=os.environ[\"AWS_REGION\"],\n",
    "                      aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "                      aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "\n",
    "embed_model = OllamaEmbedding(model_name=\"nomic-embed-text:latest\", base_url=OLLAMA_BASE_URL)\n",
    "llm = Ollama(model=\"llama3.1:latest\", base_url=OLLAMA_BASE_URL)\n",
    " \n",
    "system_message = \"\"\"You are a text network analyst working on a project to build a knowledge graph from the given text.\n",
    "The text will be passed as a dictionary where the key is the sentence number and the value is the sentence content.\n",
    "Identify at least 10 entities and 10 relationships from the given text. Must include PERSON, TECH, ORG, TEAM, and other relevant entities.\n",
    "Do not include dates and numbers. Ensure that all entities are in their lemma and lower case forms.\n",
    "The source parameter should indicate the sentence number from which the relationship is extracted.\n",
    "Provide the output strictly in the following JSON format with **only the JSON object** (no explanation or extra text)\n",
    "\"\"\"\n",
    "\n",
    "def save_txt_file(filename:str, text:str):\n",
    "    with open(filename, mode=\"w\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_json_file(filename:str, data:dict):\n",
    "    with open(filename , \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    " \n",
    "\n",
    "def generate_topic(text:str): \n",
    "    print(\"Input text: \", text)\n",
    "    document = Document(text= text)\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents([document], embed_model=embed_model)\n",
    "\n",
    "    query_engine = index.as_query_engine(llm=llm)\n",
    "    query = (\n",
    "        \"Summarize the main topic of this document in one short phrase, \"\n",
    "        \"not exceeding ten words. For example: 'Guidance on implementing Artificial Intelligence (AI) within a government organization.'\"\n",
    "    )\n",
    "\n",
    "    topic = query_engine.query(query)\n",
    "    return topic.response.replace('\"','')\n",
    "\n",
    "\n",
    "def generate_topic_with_bedrock(text:str): \n",
    "    print(\"Input text: \", text)\n",
    "     \n",
    "    user_message = f\"\"\"\n",
    "        Summarize the main topic of this paragarph: {text} in one short phrase, not exceeding ten words.\n",
    "        For example: 'Guidance on implementing Artificial Intelligence (AI) within a government organization.'\n",
    "    \"\"\"\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": user_message}],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = client.converse(\n",
    "        modelId=modelId,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\n",
    "            \"maxTokens\": 512,\n",
    "            \"temperature\": 0.5,\n",
    "            \"topP\": 0.9,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    return response_text\n",
    "\n",
    "\n",
    "def get_json_text(text: str) -> str:\n",
    "    \n",
    "    if not text.startswith(\"{\"):\n",
    "        text = text[text.index(\"{\"):]\n",
    "\n",
    "    # Ensure the sentence is closed properly\n",
    "    if text.count('\"') % 2 != 0:\n",
    "        text+= '\"'\n",
    "\n",
    "    # Ensure the JSON object is properly closed\n",
    "    if text.count(\"{\") != text.count(\"}\"):\n",
    "        if text.count(\"{\") - text.count(\"}\") == 2:\n",
    "            text += \"}\"\n",
    "        if text.count(\"[\") != text.count(\"]\"):\n",
    "            text += \"]\" \n",
    "        text += \"}\"\n",
    "        \n",
    "    return text\n",
    "\n",
    "def generate_relationships_chunk(text: str, chunk_size: int = 3000, max_tokens:int = 2000) ->tuple[list[dict[str, Any]], list[str]]:\n",
    "    \"\"\"\n",
    "    Extract entities and relationships from the provided text.\n",
    "    Reference: https://bluetickconsultants.medium.com/dual-approaches-to-building-knowledge-graphs-traditional-techniques-or-llms-400fee0f5ac9\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    user_messages = []\n",
    "    sentence_dict = {}\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if len(sentence.strip()) > 0:\n",
    "            sentence_dict[f\"sentence_{i + 1}\"] = sentence.strip()\n",
    "    \n",
    "    chunk = {}\n",
    "    current_chunk_len = 0\n",
    "    for key, sentence in sentence_dict.items():\n",
    "        sentence_len = len(sentence)\n",
    "        if current_chunk_len + sentence_len <= chunk_size:\n",
    "            chunk[key] = sentence\n",
    "            current_chunk_len += sentence_len\n",
    "        else:\n",
    "            user_messages.append({\"text\": json.dumps(chunk)})\n",
    "            chunk = {key: sentence}\n",
    "            current_chunk_len = sentence_len\n",
    "\n",
    " \n",
    "    if chunk:\n",
    "        user_messages.append({\"text\": json.dumps(chunk)})\n",
    "            \n",
    "    relationships  = []\n",
    "    fails = []\n",
    "    total_chunks = len(user_messages)\n",
    "\n",
    "    for indx, message in enumerate(user_messages):\n",
    "\n",
    "        try:\n",
    "            response = client.converse(\n",
    "                modelId=modelId,\n",
    "                messages=[{\"role\": \"user\", \"content\": [message]}],\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": max_tokens,\n",
    "                    \"temperature\": 0.5,\n",
    "                    \"topP\": 0.9,\n",
    "                },\n",
    "                system= [{\"text\": system_message}]\n",
    "            )\n",
    "            response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "            if response_text:\n",
    "                response_text = get_json_text(response_text)\n",
    "                response_json = json.loads(response_text)\n",
    "                relationships.extend(response_json[\"relationships\"])\n",
    "                print(f\"Chunk {indx + 1}/{total_chunks} processed successfully\")\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error occurred: Failed to decode JSON response\")\n",
    "            fails.append(response_text)\n",
    "            continue\n",
    "        \n",
    "    return relationships, fails\n",
    "\n",
    "\n",
    "def extract_entites(text:str)-> list[str]:\n",
    "    doc = nlp(text) \n",
    "    exculded_entites = [\"DATE\", \"LAW\", \"CARDINAL\", \"TIME\", \"ORDINAL\", \"LINK\"]\n",
    "    return [f\"{ent.text}({ent.label_})\" for ent in doc.ents if ent.label_ not in exculded_entites]\n",
    " \n",
    "\n",
    "def generate_graph_with_community_detection_from_json(\n",
    "    data: list[dict[str, Any]],\n",
    "    size_ratio: int = 1,\n",
    "    min_size: int = 10,\n",
    "    max_size: int = 100,\n",
    "    color_range=None,\n",
    "    hide_edge_label: bool = False,\n",
    "    max_clusters: Optional[int] = None,\n",
    ") -> Tuple[Graph, str, dict]:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    data (list): a relationship data in list of dictionaries.\n",
    "    size_ratio (int): Scale ratio for vertex sizes.\n",
    "    min_size (int): Minimum vertex size.\n",
    "    max_size (int): Maximum vertex size.\n",
    "    color_range (list, optional): Custom colors for communities.\n",
    "    hide_edge_label (bool): Flag to hide edge labels.\n",
    "    max_clusters (Optional[int]): Maximum number of top clusters to display. If None, show all clusters.\n",
    "   \n",
    "    Returns:\n",
    "    Tuple[Graph, str, dict]: A graph object, the most influential vertex, and a dictionary mapping clusters to topics and edges.\n",
    "    \"\"\"\n",
    "\n",
    "    entities = set()\n",
    "    relationships = set()\n",
    "    \n",
    "    \n",
    "    for rel in data:\n",
    "        relationships.add((rel['subject'], rel['predicate'], rel['object'], rel['source']))\n",
    "        entities.add(rel['subject'])\n",
    "        entities.add(rel['object'])\n",
    "    relationships = list(relationships)\n",
    "    entities = list(entities)\n",
    "    \n",
    "    vertex_sizes = {vertex: 0 for vertex in entities}\n",
    "    edge_count = {}\n",
    "    weighted_edges = []\n",
    "    edge_sentences = {} \n",
    "\n",
    "    temp_graph = Graph(directed=False)\n",
    "    temp_graph.add_vertices(entities)\n",
    "    \n",
    "    # Process the relationships and calculate weights\n",
    "    for subject, verb, obj, source in relationships:\n",
    "        edge_count[(subject, obj)] = edge_count.get((subject, obj), 0) + 1\n",
    "        \n",
    "        weight = edge_count[(subject, obj)]\n",
    "        weighted_edges.append((subject, obj, verb, weight))\n",
    "        \n",
    "        vertex_sizes[subject] += weight\n",
    "        vertex_sizes[obj] += weight\n",
    "        \n",
    "        if (subject, obj, verb) not in edge_sentences:\n",
    "            edge_sentences[(subject, obj, verb)] = []\n",
    "        edge_sentences[(subject, obj, verb)].append(source)\n",
    "    \n",
    "    # Add edges to the graph\n",
    "    temp_graph.add_edges([(subject, obj) for subject, obj, _, _ in weighted_edges])\n",
    "    temp_graph.es['weight'] = [weight for _, _, _, weight in weighted_edges]\n",
    "    \n",
    "\n",
    "    communities = temp_graph.community_multilevel(weights=temp_graph.es['weight'])\n",
    "\n",
    "    if max_clusters is not None:\n",
    "        sorted_communities = sorted(communities, key=len, reverse=True)[:max_clusters]\n",
    "        \n",
    "        filtered_nodes = set()\n",
    "        for community in sorted_communities:\n",
    "            filtered_nodes.update(temp_graph.vs[node][\"name\"] for node in community)\n",
    "\n",
    "        filtered_edges = [\n",
    "            (subject, obj, label, weight)\n",
    "            for subject, obj, label, weight in weighted_edges\n",
    "            if subject in filtered_nodes and obj in filtered_nodes\n",
    "        ]\n",
    "        final_vertices = list(filtered_nodes)\n",
    "        communities = sorted_communities\n",
    "    else:\n",
    "        filtered_edges = weighted_edges\n",
    "        final_vertices = entities\n",
    "    \n",
    "\n",
    "    final_graph = Graph(directed=False)\n",
    "    final_graph.add_vertices(final_vertices)\n",
    "    \n",
    "    labels = []\n",
    "    weights = []\n",
    "    for subject, obj, label, weight in filtered_edges:\n",
    "        final_graph.add_edge(subject, obj)\n",
    "        labels.append(label)\n",
    "        weights.append(weight)\n",
    "        \n",
    "    if not hide_edge_label:\n",
    "        final_graph.es['label'] = labels\n",
    "    final_graph.es['weight'] = weights\n",
    "\n",
    "    # Assign colors to communities\n",
    "    colors = color_range or [\n",
    "        \"green\", \"cyan\", \"orange\", \"purple\", \"magenta\", \"yellow\",\n",
    "        \"lime\", \"teal\", \"pink\", \"gold\", \"blue\", \"red\", \"maroon\", \"olive\"\n",
    "    ]\n",
    "    \n",
    "    cluster_data = {}\n",
    "    community_map = {}\n",
    "    \n",
    "    for i, community in enumerate(communities):\n",
    "        color = colors[i % len(colors)]\n",
    "        cluster_name = f\"cluster_{i + 1}\"\n",
    "        cluster_data[cluster_name] = {\"edges\": [], \"sentences\": [], \"color\": color}\n",
    "\n",
    "        for node in community:\n",
    "            community_map[temp_graph.vs[node][\"name\"]] = color\n",
    "            node_name = temp_graph.vs[node][\"name\"]\n",
    "            for edge in filtered_edges:\n",
    "                if edge[0] == node_name or edge[1] == node_name:\n",
    "                    edge_str = f\"({edge[0]}) -> {edge[2]} -> ({edge[1]})\"\n",
    "                    if edge_str not in cluster_data[cluster_name][\"edges\"]:\n",
    "                        sentence = edge_sentences.get((edge[0], edge[1], edge[2]), [])\n",
    "                        cluster_data[cluster_name][\"edges\"].append(edge_str)\n",
    "                        cluster_data[cluster_name][\"sentences\"].extend(sentence)\n",
    "    \n",
    "    # Set color and size properties for final graph\n",
    "    final_graph.vs[\"color\"] = [community_map.get(v[\"name\"], \"gray\") for v in final_graph.vs]\n",
    "    final_graph.es['color'] = [community_map.get(final_graph.vs.find(name=subject)[\"name\"], \"gray\") for subject, _, _, _ in filtered_edges]\n",
    "    \n",
    "    # Compute the most influential vertex based on vertex sizes\n",
    "    most_influential_vertex = max(vertex_sizes, key=vertex_sizes.get)\n",
    "    max_count = max(vertex_sizes.values()) if vertex_sizes else 1\n",
    "    scaled_sizes = {\n",
    "        vertex: max(min_size, min(int((size / max_count) * max_size), max_size)) * size_ratio\n",
    "        for vertex, size in vertex_sizes.items()\n",
    "    }\n",
    "    final_graph.vs[\"size\"] = [scaled_sizes.get(v[\"name\"], min_size) for v in final_graph.vs]\n",
    "    final_graph[\"title\"] = \"Entity Relationship Graph\"\n",
    "    \n",
    "    for cluster_name, data in cluster_data.items():\n",
    "        combined_text = \" \".join(data[\"sentences\"])\n",
    "        topic = \"\"  # Placeholder for topic generation logic\n",
    "        data[\"topic\"] = topic\n",
    "\n",
    "    return final_graph, most_influential_vertex, cluster_data\n",
    "\n",
    "\n",
    "def visualize_graph(graph: Graph, influential_vertex: str, layout_algorithm:str = \"fruchterman_reingold\", output_file=\"graph_with_center.png\", bbox:int= 1200):\n",
    "    \"\"\"\n",
    "    Visualize the graph and ensure the most influential vertex is at the center.\n",
    "\n",
    "    Args:\n",
    "        graph (Graph): iGraph graph object.\n",
    "        influential_vertex (str): The name of the most influential vertex.\n",
    "        output_file (str): Path to save the visualization.\n",
    "    \"\"\"\n",
    "    \n",
    "    layout = graph.layout(layout_algorithm)\n",
    " \n",
    "    # influential_index = graph.vs.find(name=influential_vertex).index\n",
    "    \n",
    "    # placing most influential node as center of the graph\n",
    "    # layout[influential_index] = [0, 0]  \n",
    "\n",
    "    scaled_layout = [[coord * 1.5 for coord in pos] for pos in layout]\n",
    "\n",
    "    plot(\n",
    "        graph,\n",
    "        layout=scaled_layout,\n",
    "        vertex_size=graph.vs[\"size\"],\n",
    "        vertex_color=graph.vs[\"color\"],\n",
    "        vertex_label=graph.vs[\"name\"],\n",
    "        edge_width=[weight for weight in graph.es[\"weight\"]],\n",
    "        edge_color=graph.es['color'],\n",
    "        bbox=(bbox, bbox),\n",
    "        margin=80,\n",
    "        target=output_file,\n",
    "        main= graph[\"title\"]\n",
    "        \n",
    "    )\n",
    " \n",
    "    print(f\"Graph saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found All_clusters/public-benefits-and-ai/public-benefits-and-ai(Gliner + Spacy)(org).txt\n"
     ]
    }
   ],
   "source": [
    "# reuse local extracted text file\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def read_file(filepath: str) -> str:\n",
    "    with open(filepath, mode=\"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "txt_folder = pdf_filename.replace(\".pdf\", \"\")\n",
    "\n",
    "file_pattern = f\"All_clusters/{txt_folder}/*(org).txt\"\n",
    "matching_files = glob.glob(file_pattern)\n",
    "\n",
    "if len(matching_files) == 1:\n",
    "    print(f\"Found {matching_files[0]}\")\n",
    "    text = read_file(matching_files[0])\n",
    "else:\n",
    "    raise FileNotFoundError(f\"No file matching the pattern {file_pattern} found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the larger the chunk, the processing time will be fast but the accuracy will be low\n",
    "relationships, fails = generate_relationships_chunk(text, max_tokens=1500, chunk_size = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_txt_file(f\"{pdf_filename.replace(\".pdf\", \"\")}(debug).py\", str(relationships))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved as public-benefits-and-ai(LLM-based)/knowledge-graph (Top 5 Clusters).png\n"
     ]
    }
   ],
   "source": [
    "max_clusters =  5\n",
    "graph, most_influential_node, topic_clusters  = generate_graph_with_community_detection_from_json(relationships, max_clusters=max_clusters,  min_size=20, max_size=100, size_ratio=1, hide_edge_label=True)\n",
    "\n",
    "layout_algorithms = [\"fruchterman_reingold\"]\n",
    " \n",
    "folder_name = pdf_filename.replace(\".pdf\", \"\")\n",
    "\n",
    "folder_name += \"(LLM-based)\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "clusters = f\"Top {max_clusters} Clusters\" if max_clusters is not None else \"All Clusters\"\n",
    " \n",
    "for layout_algorithm in layout_algorithms:\n",
    "    output_file = f\"{folder_name}/knowledge-graph ({clusters}).png\"\n",
    "    visualize_graph(graph, most_influential_node,  layout_algorithm=layout_algorithm,output_file=output_file, bbox=7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run to save analysis outcome\"\"\"\n",
    " \n",
    "entities = set()\n",
    "entities_types = set()\n",
    "readable_edges = []\n",
    "\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "user_messages =[]\n",
    "sentence_dict = {}\n",
    "\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    if len(sentence.strip()) > 0:\n",
    "        sentence_dict[f\"sentence_{i + 1}\"] = sentence.strip()\n",
    " \n",
    "save_txt_file(\"urgent.py\", str(sentence_dict))\n",
    " \n",
    "for edge in relationships:\n",
    "    subj = edge[\"subject\"]\n",
    "    obj = edge[\"object\"]\n",
    "    entities.add(subj)\n",
    "    entities.add(obj)\n",
    "    entities_types.add(edge[\"subjectType\"])\n",
    "    entities_types.add(edge[\"objectType\"])\n",
    "    predicate = edge[\"predicate\"]\n",
    "    readable_edges.append(f\"({subj})->{predicate}->({obj})\")\n",
    "        \n",
    "   \n",
    "\n",
    "topic_md = \"\"\n",
    " \n",
    "for cluster_name, cluster in topic_clusters.items():\n",
    "    keys = cluster[\"sentences\"]\n",
    "    combined_text = \"\"\n",
    "    sentences = []\n",
    "    for key in keys:\n",
    "        sentence = sentence_dict[key]\n",
    "        sentences.append(sentence)\n",
    "    # replace teh sentences' keys with the actual sentences\n",
    "    cluster[\"sentences\"] = sentences\n",
    "    \n",
    "    # remove duplicates to generate a topic\n",
    "    sentences = list(set(sentences))\n",
    "    combined_text = \". \".join(sentences)\n",
    " \n",
    "    topic = \"\"\n",
    "    if combined_text:\n",
    "        print(f\"Generating topic for {cluster_name}\")\n",
    "        while True:\n",
    "            try:\n",
    "                topic = generate_topic_with_bedrock(combined_text) \n",
    "            except TimeoutError:\n",
    "                pass\n",
    "            if topic:\n",
    "                topic_md += f\"# {topic}\\n\"\n",
    "                topic_md += f\"### {cluster_name} ({cluster['color']})\\n\"\n",
    "                topic_md += f\"- {combined_text}\"\n",
    "                topic_md += \"\\n\\n\"\n",
    "                break\n",
    "    cluster[\"topic\"] = topic\n",
    "\n",
    "\n",
    "metadata = {\n",
    "    \"vertices\" : list(entities),\n",
    "    \"edges\": readable_edges,\n",
    "    \"most_influential_node\": most_influential_node,\n",
    "    \"clusters\": {**topic_clusters},\n",
    "    \"captured_entities\": list(entities_types)\n",
    "}\n",
    "\n",
    "save_txt_file(f\"{folder_name}/{folder_name}(org).txt\", text)\n",
    "try:\n",
    "    save_txt_file(f\"{folder_name}/{folder_name}({clusters}' topics).md\", topic_md)\n",
    "except Exception:\n",
    "    # some text are only included when working with neuralcoref\n",
    "    pass\n",
    "\n",
    "save_json_file(f\"{folder_name}/{folder_name}-metadata ({clusters}).json\", metadata)\n",
    " \n",
    "print(f\"All data are saved at {folder_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
