{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from docling.document_converter import DocumentConverter\n",
    "load_dotenv() \n",
    "\n",
    "\n",
    "files = [\n",
    "    \"Hotel Marketing.pdf\"\n",
    "]\n",
    "\n",
    "pdf_filename =  files[0]\n",
    "\n",
    "\n",
    "\"\"\" Testing: markdown extraction with docling\"\"\"\n",
    "print(torch.cuda.is_available())   \n",
    " \n",
    "\n",
    "def write_file(filename: str, content: str):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    " \n",
    "attempt = 5\n",
    "\n",
    "FOLDER = \"output(docling)\"\n",
    "Path(FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "table_data = []\n",
    "\n",
    "for filename in files:\n",
    "    filepath = f\"pdf/{filename}\"\n",
    "    \n",
    "    time_taken_list = []\n",
    "    for _ in range(attempt):   \n",
    "        start_time = datetime.now()\n",
    "\n",
    "        converter = DocumentConverter()\n",
    "        result = converter.convert(filepath)\n",
    "        markdown_content = result.document.export_to_markdown()\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        time_taken = end_time - start_time\n",
    "        time_taken_list.append(time_taken)\n",
    "\n",
    "        md_filename = filename.replace(\".pdf\", \".md\")\n",
    "        write_file(f\"{FOLDER}/{md_filename}\", markdown_content)\n",
    " \n",
    "    avg_time_taken = sum(time_taken_list, timedelta()) / len(time_taken_list)\n",
    "    table_data.append((filename, time_taken_list, avg_time_taken))\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing hasn't finished yet. Please try later\n"
     ]
    }
   ],
   "source": [
    "markdown_table = \"| File Name                                    | Time Taken (1st) | Time Taken (2nd) | Time Taken (3rd) | Time Taken (4th) | Time Taken (5th) | Average Time Taken |\\n\"\n",
    "markdown_table += \"|----------------------------------------------|------------------|------------------|------------------|------------------|------------------|--------------------|\\n\"\n",
    "\n",
    "for file, time_taken_list, avg_time_taken in table_data:\n",
    "    time_taken_str = \" | \".join([get_time_str(time) for time in time_taken_list])\n",
    "    markdown_table += f\"| {file} | {time_taken_str} | {get_time_str(avg_time_taken)} |\\n\"\n",
    "\n",
    "print(markdown_table)\n",
    "\n",
    "write_file(f\"{FOLDER}/time_taken_summary.md\", markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU with PyTorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/hm3/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import torch \n",
    "import openai\n",
    "from igraph import Graph, plot\n",
    "from typing import Tuple, Optional, Any\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from dotenv import load_dotenv\n",
    " \n",
    "load_dotenv() \n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU with PyTorch\")\n",
    " \n",
    "region_name = os.environ.get(\"AWS_REGION\") \n",
    "aws_access_key_id = os.environ.get(\"AWS_ACCESS_KEY_ID\") \n",
    "aws_secret_access_key = os.environ.get(\"AWS_SECRET_ACCESS_KEY\") \n",
    "\n",
    "bedrock_model = os.environ.get(\"BEDROCK_MODEL_NAME\")\n",
    "modelId = os.environ[\"BEDROCK_MODEL_NAME\"]\n",
    "LLM_BASE_URL = os.environ[\"LLM_BASE_URL\"]\n",
    "LLM_MODEL=os.environ[\"LLM_MODEL\"]\n",
    "\n",
    "client = boto3.client('bedrock-runtime', \n",
    "                      region_name=os.environ[\"AWS_REGION\"],\n",
    "                      aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "                      aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "\n",
    " \n",
    "system_message = \"\"\"You are a text network analyst working on a project to build a knowledge graph from the given text.\n",
    "The text will be passed as a dictionary where the key is the sentence number and the value is the sentence content.\n",
    "Identify at least 10 entities and 10 relationships from the given text. Must include PERSON, TECH, ORG, TEAM, and other relevant entities.\n",
    "Do not include dates and numbers. Ensure that all entities are in their lemma and lower case forms.\n",
    "The source parameter should indicate the sentence number from which the relationship is extracted.\n",
    "Provide the output strictly in the following JSON format with **only the JSON object** (no explanation or extra text):\n",
    "   {\n",
    "    \"relationships\": [\n",
    "        {\n",
    "            \"subject\": \"Entity1\",\n",
    "            \"subjectType\": \"Person\",\n",
    "            \"predicate\": \"works_for\",\n",
    "            \"object\": \"Entity2\",\n",
    "            \"objectType\": \"Organization\",\n",
    "            \"source\": \"sentence_1\"\n",
    "        },\n",
    "        {\n",
    "            \"subject\": \"Policy1\",\n",
    "            \"subjectType\": \"Policy\",\n",
    "            \"predicate\": \"focuses_on\",\n",
    "            \"object\": \"Objective1\",\n",
    "            \"objectType\": \"Location\",\n",
    "            \"source\": \"sentence_2.\"\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def save_txt_file(filename:str, text:str):\n",
    "    with open(filename, mode=\"w\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_json_file(filename:str, data:dict):\n",
    "    with open(filename , \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    " \n",
    "\n",
    "def generate_topic_with_deepseek(text:str): \n",
    "    client = openai.Client(\n",
    "        base_url=f\"{LLM_BASE_URL}/v1\",\n",
    "        api_key=\"ollama\"  \n",
    "    )\n",
    "    user_message = f\"\"\"\n",
    "        Summarize the main topic of this paragarph: {text} in one short phrase, not exceeding ten words.\n",
    "        For example: 'Guidance on implementing Artificial Intelligence (AI) within a government organization.'\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[\n",
    "                    {\"role\": \"user\", \"content\": user_message},\n",
    "            ],\n",
    "            stream=False,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def generate_topic_with_bedrock(text:str): \n",
    "    print(\"Input text: \", text)\n",
    "     \n",
    "    user_message = f\"\"\"\n",
    "        Summarize the main topic of this paragarph: {text} in one short phrase, not exceeding ten words.\n",
    "        For example: 'Guidance on implementing Artificial Intelligence (AI) within a government organization.'\n",
    "    \"\"\"\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": user_message}],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = client.converse(\n",
    "        modelId=modelId,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\n",
    "            \"maxTokens\": 512,\n",
    "            \"temperature\": 0.5,\n",
    "            \"topP\": 0.9,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    return response_text\n",
    "\n",
    "\n",
    "def get_json_text(text: str) -> str:\n",
    "    \n",
    "    if not text.startswith(\"{\"):\n",
    "        text = text[text.index(\"{\"):]\n",
    "\n",
    "    # Ensure the sentence is closed properly\n",
    "    if text.count('\"') % 2 != 0:\n",
    "        text+= '\"'\n",
    "\n",
    "    # Ensure the JSON object is properly closed\n",
    "    if text.count(\"{\") != text.count(\"}\"):\n",
    "        if text.count(\"{\") - text.count(\"}\") == 2:\n",
    "            text += \"}\"\n",
    "        if text.count(\"[\") != text.count(\"]\"):\n",
    "            text += \"]\" \n",
    "        text += \"}\"\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_relationships_deepseek(text:str, chunk_size:int = 2400, max_tokens:int = 1000):\n",
    "    \n",
    "    client = openai.Client(\n",
    "        base_url=f\"{LLM_BASE_URL}/v1\",\n",
    "        api_key=\"ollama\"  \n",
    "    )\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    user_messages = []\n",
    "    sentence_dict = {}\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if len(sentence.strip()) > 0:\n",
    "            sentence_dict[f\"sentence_{i + 1}\"] = sentence.strip()\n",
    "    \n",
    "    chunk = {}\n",
    "    current_chunk_len = 0\n",
    "    for key, sentence in sentence_dict.items():\n",
    "        sentence_len = len(sentence)\n",
    "        if current_chunk_len + sentence_len <= chunk_size:\n",
    "            chunk[key] = sentence\n",
    "            current_chunk_len += sentence_len\n",
    "        else:\n",
    "            user_messages.append( json.dumps(chunk))\n",
    "            chunk = {key: sentence}\n",
    "            current_chunk_len = sentence_len\n",
    "\n",
    " \n",
    "    if chunk:\n",
    "        user_messages.append(json.dumps(chunk))\n",
    "            \n",
    "    relationships  = []\n",
    "    fails = []\n",
    "    total_chunks = len(user_messages)\n",
    "\n",
    "    for indx, message in enumerate(user_messages):\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\": message},\n",
    "                ],\n",
    "                stream=False,\n",
    "                max_tokens=max_tokens \n",
    "            )\n",
    "\n",
    "            response_text = response.choices[0].message.content\n",
    "            if response_text:\n",
    "                response_text = get_json_text(response_text)\n",
    "                response_json = json.loads(response_text)\n",
    "                \n",
    "                print(f\"Successfuly prossed {indx}/{total_chunks}...\")\n",
    "                relationships.append(response_json[\"relationships\"])\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error occurred: Failed to decode JSON response\")\n",
    "            fails.append(response_text) # for debugging\n",
    "            continue\n",
    "    \n",
    "    return relationships\n",
    "\n",
    "def generate_relationships_chunk_batch(text: str, chunk_size: int = 2400, max_tokens:int = 1000) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract entities and relationships from the provided text.\n",
    "    Reference: https://bluetickconsultants.medium.com/dual-approaches-to-building-knowledge-graphs-traditional-techniques-or-llms-400fee0f5ac9\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    current_chunk = \"\"\n",
    "    user_messages =[]\n",
    "    \n",
    "    # to ensure the response is return as complete\n",
    "    sentence_token_limit = 2000\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) <= sentence_token_limit:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            user_messages.append({\"text\": current_chunk.strip()})\n",
    "            current_chunk = sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        user_messages.append({\"text\": current_chunk.strip()})\n",
    "    \n",
    "    try:\n",
    "        response = client.converse(\n",
    "            modelId=modelId,\n",
    "            messages=[{\"role\": \"user\", \"content\": user_messages}],\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": max_tokens,\n",
    "                \"temperature\": 0.5,\n",
    "                \"topP\": 0.9,\n",
    "            },\n",
    "            system= [{\"text\": system_message}]\n",
    "        )\n",
    "        response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        if response_text:\n",
    "            response_text = get_json_text(response_text)\n",
    "            response_json = json.loads(response_text)\n",
    "            return response_json[\"relationships\"]\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error occurred: Failed to decode JSON response\")\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "def generate_relationships_chunk(text: str, chunk_size: int = 3000, max_tokens:int = 2000) ->tuple[list[dict[str, Any]], list[str]]:\n",
    "    \"\"\"\n",
    "    Extract entities and relationships from the provided text.\n",
    "    Reference: https://bluetickconsultants.medium.com/dual-approaches-to-building-knowledge-graphs-traditional-techniques-or-llms-400fee0f5ac9\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    user_messages = []\n",
    "    sentence_dict = {}\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if len(sentence.strip()) > 0:\n",
    "            sentence_dict[f\"sentence_{i + 1}\"] = sentence.strip()\n",
    "    \n",
    "    chunk = {}\n",
    "    current_chunk_len = 0\n",
    "    for key, sentence in sentence_dict.items():\n",
    "        sentence_len = len(sentence)\n",
    "        if current_chunk_len + sentence_len <= chunk_size:\n",
    "            chunk[key] = sentence\n",
    "            current_chunk_len += sentence_len\n",
    "        else:\n",
    "            user_messages.append({\"text\": json.dumps(chunk)})\n",
    "            chunk = {key: sentence}\n",
    "            current_chunk_len = sentence_len\n",
    "\n",
    " \n",
    "    if chunk:\n",
    "        user_messages.append({\"text\": json.dumps(chunk)})\n",
    "            \n",
    "    relationships  = []\n",
    "    fails = []\n",
    "    total_chunks = len(user_messages)\n",
    "\n",
    "    for indx, message in enumerate(user_messages):\n",
    "\n",
    "        try:\n",
    "            response = client.converse(\n",
    "                modelId=modelId,\n",
    "                messages=[{\"role\": \"user\", \"content\": [message]}],\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": max_tokens,\n",
    "                    \"temperature\": 0.5,\n",
    "                    \"topP\": 0.9,\n",
    "                },\n",
    "                system= [{\"text\": system_message}]\n",
    "            )\n",
    "            response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "            if response_text:\n",
    "                response_text = get_json_text(response_text)\n",
    "                response_json = json.loads(response_text)\n",
    "                relationships.extend(response_json[\"relationships\"])\n",
    "                print(f\"Chunk {indx + 1}/{total_chunks} processed successfully\")\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error occurred: Failed to decode JSON response\")\n",
    "            fails.append(response_text)\n",
    "            continue\n",
    "        \n",
    "    return relationships, fails\n",
    "\n",
    "\n",
    "def generate_graph_with_community_detection_from_json(\n",
    "    data: list[dict[str, Any]],\n",
    "    size_ratio: int = 1,\n",
    "    min_size: int = 10,\n",
    "    max_size: int = 100,\n",
    "    color_range=None,\n",
    "    hide_edge_label: bool = False,\n",
    "    max_clusters: Optional[int] = None,\n",
    ") -> Tuple[Graph, str, dict]:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    data (list): a relationship data in list of dictionaries.\n",
    "    size_ratio (int): Scale ratio for vertex sizes.\n",
    "    min_size (int): Minimum vertex size.\n",
    "    max_size (int): Maximum vertex size.\n",
    "    color_range (list, optional): Custom colors for communities.\n",
    "    hide_edge_label (bool): Flag to hide edge labels.\n",
    "    max_clusters (Optional[int]): Maximum number of top clusters to display. If None, show all clusters.\n",
    "   \n",
    "    Returns:\n",
    "    Tuple[Graph, str, dict]: A graph object, the most influential vertex, and a dictionary mapping clusters to topics and edges.\n",
    "    \"\"\"\n",
    "\n",
    "    entities = set()\n",
    "    relationships = set()\n",
    "    \n",
    "    \n",
    "    for rel in data:\n",
    "        relationships.add((rel['subject'], rel['predicate'], rel['object'], rel['source']))\n",
    "        entities.add(rel['subject'])\n",
    "        entities.add(rel['object'])\n",
    "    relationships = list(relationships)\n",
    "    entities = list(entities)\n",
    "    \n",
    "    vertex_sizes = {vertex: 0 for vertex in entities}\n",
    "    edge_count = {}\n",
    "    weighted_edges = []\n",
    "    edge_sentences = {} \n",
    "\n",
    "    temp_graph = Graph(directed=False)\n",
    "    temp_graph.add_vertices(entities)\n",
    "    \n",
    "    # Process the relationships and calculate weights\n",
    "    for subject, verb, obj, source in relationships:\n",
    "        edge_count[(subject, obj)] = edge_count.get((subject, obj), 0) + 1\n",
    "        \n",
    "        weight = edge_count[(subject, obj)]\n",
    "        weighted_edges.append((subject, obj, verb, weight))\n",
    "        \n",
    "        vertex_sizes[subject] += weight\n",
    "        vertex_sizes[obj] += weight\n",
    "        \n",
    "        if (subject, obj, verb) not in edge_sentences:\n",
    "            edge_sentences[(subject, obj, verb)] = []\n",
    "        edge_sentences[(subject, obj, verb)].append(source)\n",
    "    \n",
    "    # Add edges to the graph\n",
    "    temp_graph.add_edges([(subject, obj) for subject, obj, _, _ in weighted_edges])\n",
    "    temp_graph.es['weight'] = [weight for _, _, _, weight in weighted_edges]\n",
    "    \n",
    "\n",
    "    communities = temp_graph.community_multilevel(weights=temp_graph.es['weight'])\n",
    "\n",
    "    if max_clusters is not None:\n",
    "        sorted_communities = sorted(communities, key=len, reverse=True)[:max_clusters]\n",
    "        \n",
    "        filtered_nodes = set()\n",
    "        for community in sorted_communities:\n",
    "            filtered_nodes.update(temp_graph.vs[node][\"name\"] for node in community)\n",
    "\n",
    "        filtered_edges = [\n",
    "            (subject, obj, label, weight)\n",
    "            for subject, obj, label, weight in weighted_edges\n",
    "            if subject in filtered_nodes and obj in filtered_nodes\n",
    "        ]\n",
    "        final_vertices = list(filtered_nodes)\n",
    "        communities = sorted_communities\n",
    "    else:\n",
    "        filtered_edges = weighted_edges\n",
    "        final_vertices = entities\n",
    "    \n",
    "\n",
    "    final_graph = Graph(directed=False)\n",
    "    final_graph.add_vertices(final_vertices)\n",
    "    \n",
    "    labels = []\n",
    "    weights = []\n",
    "    for subject, obj, label, weight in filtered_edges:\n",
    "        final_graph.add_edge(subject, obj)\n",
    "        labels.append(label)\n",
    "        weights.append(weight)\n",
    "        \n",
    "    if not hide_edge_label:\n",
    "        final_graph.es['label'] = labels\n",
    "    final_graph.es['weight'] = weights\n",
    "\n",
    "    # Assign colors to communities\n",
    "    colors = color_range or [\n",
    "        \"green\", \"cyan\", \"orange\", \"purple\", \"magenta\", \"yellow\",\n",
    "        \"lime\", \"teal\", \"pink\", \"gold\", \"blue\", \"red\", \"maroon\", \"olive\"\n",
    "    ]\n",
    "    \n",
    "    cluster_data = {}\n",
    "    community_map = {}\n",
    "    \n",
    "    for i, community in enumerate(communities):\n",
    "        color = colors[i % len(colors)]\n",
    "        cluster_name = f\"cluster_{i + 1}\"\n",
    "        cluster_data[cluster_name] = {\"edges\": [], \"sentences\": [], \"color\": color}\n",
    "\n",
    "        for node in community:\n",
    "            community_map[temp_graph.vs[node][\"name\"]] = color\n",
    "            node_name = temp_graph.vs[node][\"name\"]\n",
    "            for edge in filtered_edges:\n",
    "                if edge[0] == node_name or edge[1] == node_name:\n",
    "                    edge_str = f\"({edge[0]}) -> {edge[2]} -> ({edge[1]})\"\n",
    "                    if edge_str not in cluster_data[cluster_name][\"edges\"]:\n",
    "                        sentence = edge_sentences.get((edge[0], edge[1], edge[2]), [])\n",
    "                        cluster_data[cluster_name][\"edges\"].append(edge_str)\n",
    "                        cluster_data[cluster_name][\"sentences\"].extend(sentence)\n",
    "    \n",
    "    # Set color and size properties for final graph\n",
    "    final_graph.vs[\"color\"] = [community_map.get(v[\"name\"], \"gray\") for v in final_graph.vs]\n",
    "    final_graph.es['color'] = [community_map.get(final_graph.vs.find(name=subject)[\"name\"], \"gray\") for subject, _, _, _ in filtered_edges]\n",
    "    \n",
    "    # Compute the most influential vertex based on vertex sizes\n",
    "    most_influential_vertex = max(vertex_sizes, key=vertex_sizes.get)\n",
    "    max_count = max(vertex_sizes.values()) if vertex_sizes else 1\n",
    "    scaled_sizes = {\n",
    "        vertex: max(min_size, min(int((size / max_count) * max_size), max_size)) * size_ratio\n",
    "        for vertex, size in vertex_sizes.items()\n",
    "    }\n",
    "    final_graph.vs[\"size\"] = [scaled_sizes.get(v[\"name\"], min_size) for v in final_graph.vs]\n",
    "    final_graph[\"title\"] = \"Entity Relationship Graph\"\n",
    "    \n",
    "    for cluster_name, data in cluster_data.items():\n",
    "        combined_text = \" \".join(data[\"sentences\"])\n",
    "        topic = \"\"  # Placeholder for topic generation logic\n",
    "        data[\"topic\"] = topic\n",
    "\n",
    "    return final_graph, most_influential_vertex, cluster_data\n",
    "\n",
    "\n",
    "def visualize_graph(graph: Graph, influential_vertex: str, layout_algorithm:str = \"fruchterman_reingold\", output_file=\"graph_with_center.png\", bbox:int= 1200):\n",
    "    \"\"\"\n",
    "    Visualize the graph and ensure the most influential vertex is at the center.\n",
    "\n",
    "    Args:\n",
    "        graph (Graph): iGraph graph object.\n",
    "        influential_vertex (str): The name of the most influential vertex.\n",
    "        output_file (str): Path to save the visualization.\n",
    "    \"\"\"\n",
    "    \n",
    "    layout = graph.layout(layout_algorithm)\n",
    " \n",
    "    # influential_index = graph.vs.find(name=influential_vertex).index\n",
    "    \n",
    "    # placing most influential node as center of the graph\n",
    "    # layout[influential_index] = [0, 0]  \n",
    "\n",
    "    scaled_layout = [[coord * 1.5 for coord in pos] for pos in layout]\n",
    "\n",
    "    plot(\n",
    "        graph,\n",
    "        layout=scaled_layout,\n",
    "        vertex_size=graph.vs[\"size\"],\n",
    "        vertex_color=graph.vs[\"color\"],\n",
    "        vertex_label=graph.vs[\"name\"],\n",
    "        edge_width=[weight for weight in graph.es[\"weight\"]],\n",
    "        edge_color=graph.es['color'],\n",
    "        bbox=(bbox, bbox),\n",
    "        margin=80,\n",
    "        target=output_file,\n",
    "        main= graph[\"title\"]\n",
    "        \n",
    "    )\n",
    " \n",
    "    print(f\"Graph saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found All_clusters/public-benefits-and-ai/public-benefits-and-ai(Gliner + Spacy)(org).txt\n"
     ]
    }
   ],
   "source": [
    "# reuse local extracted text file\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def read_file(filepath: str) -> str:\n",
    "    with open(filepath, mode=\"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "txt_folder = pdf_filename.replace(\".pdf\", \"\")\n",
    "\n",
    "file_pattern = f\"All_clusters/{txt_folder}/*(org).txt\"\n",
    "matching_files = glob.glob(file_pattern)\n",
    "\n",
    "if len(matching_files) == 1:\n",
    "    print(f\"Found {matching_files[0]}\")\n",
    "    text = read_file(matching_files[0])\n",
    "else:\n",
    "    raise FileNotFoundError(f\"No file matching the pattern {file_pattern} found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the larger the chunk, the processing time will be fast but the accuracy will be low\n",
    "relationships, fails = generate_relationships_chunk(text, max_tokens=1500, chunk_size = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_txt_file(f\"{pdf_filename.replace(\".pdf\", \"\")}(debug).py\", str(relationships))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved as public-benefits-and-ai(LLM-based)/knowledge-graph (Top 5 Clusters).png\n"
     ]
    }
   ],
   "source": [
    "max_clusters =  5\n",
    "graph, most_influential_node, topic_clusters  = generate_graph_with_community_detection_from_json(relationships, max_clusters=max_clusters,  min_size=20, max_size=100, size_ratio=1, hide_edge_label=True)\n",
    "\n",
    "layout_algorithms = [\"fruchterman_reingold\"]\n",
    " \n",
    "folder_name = pdf_filename.replace(\".pdf\", \"\")\n",
    "\n",
    "folder_name += \"(LLM-based)\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "clusters = f\"Top {max_clusters} Clusters\" if max_clusters is not None else \"All Clusters\"\n",
    " \n",
    "for layout_algorithm in layout_algorithms:\n",
    "    output_file = f\"{folder_name}/knowledge-graph ({clusters}).png\"\n",
    "    visualize_graph(graph, most_influential_node,  layout_algorithm=layout_algorithm,output_file=output_file, bbox=7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run to save analysis outcome\"\"\"\n",
    " \n",
    "entities = set()\n",
    "entities_types = set()\n",
    "readable_edges = []\n",
    "\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "user_messages =[]\n",
    "sentence_dict = {}\n",
    "\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    if len(sentence.strip()) > 0:\n",
    "        sentence_dict[f\"sentence_{i + 1}\"] = sentence.strip()\n",
    " \n",
    "save_txt_file(\"urgent.py\", str(sentence_dict))\n",
    " \n",
    "for edge in relationships:\n",
    "    subj = edge[\"subject\"]\n",
    "    obj = edge[\"object\"]\n",
    "    entities.add(subj)\n",
    "    entities.add(obj)\n",
    "    entities_types.add(edge[\"subjectType\"])\n",
    "    entities_types.add(edge[\"objectType\"])\n",
    "    predicate = edge[\"predicate\"]\n",
    "    readable_edges.append(f\"({subj})->{predicate}->({obj})\")\n",
    "        \n",
    "   \n",
    "\n",
    "topic_md = \"\"\n",
    " \n",
    "for cluster_name, cluster in topic_clusters.items():\n",
    "    keys = cluster[\"sentences\"]\n",
    "    combined_text = \"\"\n",
    "    sentences = []\n",
    "    for key in keys:\n",
    "        sentence = sentence_dict[key]\n",
    "        sentences.append(sentence)\n",
    "    # replace teh sentences' keys with the actual sentences\n",
    "    cluster[\"sentences\"] = sentences\n",
    "    \n",
    "    # remove duplicates to generate a topic\n",
    "    sentences = list(set(sentences))\n",
    "    combined_text = \". \".join(sentences)\n",
    " \n",
    "    topic = \"\"\n",
    "    if combined_text:\n",
    "        print(f\"Generating topic for {cluster_name}\")\n",
    "        while True:\n",
    "            try:\n",
    "                topic = generate_topic_with_bedrock(combined_text) \n",
    "            except TimeoutError:\n",
    "                pass\n",
    "            if topic:\n",
    "                topic_md += f\"# {topic}\\n\"\n",
    "                topic_md += f\"### {cluster_name} ({cluster['color']})\\n\"\n",
    "                topic_md += f\"- {combined_text}\"\n",
    "                topic_md += \"\\n\\n\"\n",
    "                break\n",
    "    cluster[\"topic\"] = topic\n",
    "\n",
    "\n",
    "metadata = {\n",
    "    \"vertices\" : list(entities),\n",
    "    \"edges\": readable_edges,\n",
    "    \"most_influential_node\": most_influential_node,\n",
    "    \"clusters\": {**topic_clusters},\n",
    "    \"captured_entities\": list(entities_types)\n",
    "}\n",
    "\n",
    "save_txt_file(f\"{folder_name}/{folder_name}(org).txt\", text)\n",
    "try:\n",
    "    save_txt_file(f\"{folder_name}/{folder_name}({clusters}' topics).md\", topic_md)\n",
    "except Exception:\n",
    "    # some text are only included when working with neuralcoref\n",
    "    pass\n",
    "\n",
    "save_json_file(f\"{folder_name}/{folder_name}-metadata ({clusters}).json\", metadata)\n",
    " \n",
    "print(f\"All data are saved at {folder_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, I need to help this user by summarizing a paragarph into a single phrase. The original paragraph talks about a pull request introducing a new caching mechanism for metadata retrieval. The main goal is reduce database load and speed up response times.\n",
      "\n",
      "Hmm, the user specifically asked for it in one short phrase, not exceeding ten words. They also gave an example: 'Guidance on implementing Artificial Intelligence (AI) within a government organization.' So I should follow that structure but instead use the relevant terms for caching and metadata.\n",
      "\n",
      "Let me look at the key points again: new caching mechanism, reduces database load, speeds up response times, frequently accessed metadata. The focus is on performance improvements through caching. Maybe something like \"Implementing a caching mechanism to reduce database load...\" That captures the action and the benefit clearly.\n",
      "</think>\n",
      "\n",
      "Implementing a caching mechanism to reduce database load and speed up response times for frequently accessed metadata.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sample_text = \"This pull request implements a new caching mechanism for metadata retrieval in the application. The goal is to reduce database load and speed up response times for frequently accessed metadata.\"\n",
    "topic = generate_topic_with_deepseek(sample_text)\n",
    "print(topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "response = topic\n",
    "# Extract the <think> section\n",
    "think_match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)\n",
    "think_section = think_match.group(1).strip() if think_match else None\n",
    "\n",
    "# Extract the translated result\n",
    "translated_result = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()\n",
    "\n",
    "# print(\"Think Section:\")\n",
    "# print(think_section)\n",
    "print(\"\\nTranslated Result:\")\n",
    "print(translated_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
